#!/usr/bin/env python3
import requests
import sqlite3
import json
import time
import hashlib
import logging
from datetime import datetime, timedelta
from pathlib import Path
import feedparser
from bs4 import BeautifulSoup
import praw
import subprocess
import os
from urllib.parse import urljoin, urlparse
import re
from typing import Dict, List, Optional
import warnings
warnings.filterwarnings('ignore')

class NewsSourceManager:
    """ë‰´ìŠ¤ ì†ŒìŠ¤ ê´€ë¦¬ì"""

    def __init__(self):
        # API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì„¤ì •)
        self.newsapi_key = os.getenv('NEWSAPI_KEY', '')  # NewsAPI.org í‚¤

        # Reddit ì„¤ì •
        self.reddit = None
        self.setup_reddit()

        # í—¤ë” ì„¤ì • (ì›¹ ìŠ¤í¬ë˜í•‘ìš©)
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        # í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œ
        self.korean_rss_feeds = {
            'naver_finance': 'https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258',
            'hankyung': 'https://www.hankyung.com/feed/stock',
            'maeil': 'https://www.mk.co.kr/rss/30300001/',
            'seoul_finance': 'http://www.sedaily.com/RSS/S11.xml'
        }

        # ì¢…ëª© í‚¤ì›Œë“œ ë§¤í•‘
        self.symbol_keywords = {
            # ë¯¸êµ­ ì£¼ì‹
            'AAPL': ['Apple', 'iPhone', 'iPad', 'Mac', 'Tim Cook', 'ì• í”Œ'],
            'MSFT': ['Microsoft', 'Windows', 'Azure', 'Office', 'Teams', 'ë§ˆì´í¬ë¡œì†Œí”„íŠ¸'],
            'GOOGL': ['Google', 'Alphabet', 'YouTube', 'Android', 'Chrome', 'êµ¬ê¸€'],
            'AMZN': ['Amazon', 'AWS', 'Alexa', 'Prime', 'Bezos', 'ì•„ë§ˆì¡´'],
            'TSLA': ['Tesla', 'Elon Musk', 'Model', 'electric vehicle', 'EV', 'í…ŒìŠ¬ë¼'],
            'META': ['Meta', 'Facebook', 'Instagram', 'WhatsApp', 'Metaverse', 'ë©”íƒ€'],
            'NVDA': ['NVIDIA', 'GPU', 'AI chip', 'gaming', 'datacenter', 'ì—”ë¹„ë””ì•„'],

            # í•œêµ­ ì£¼ì‹
            '005930.KS': ['ì‚¼ì„±ì „ì', 'Samsung Electronics', 'ê°¤ëŸ­ì‹œ', 'Galaxy', 'ë°˜ë„ì²´', 'ìŠ¤ë§ˆíŠ¸í°'],
            '000660.KS': ['SKí•˜ì´ë‹‰ìŠ¤', 'SK Hynix', 'ë©”ëª¨ë¦¬', 'DRAM', 'NAND'],
            '035420.KS': ['NAVER', 'ë„¤ì´ë²„', 'ë¼ì¸', 'Line', 'ì›¹íˆ°', 'ê²€ìƒ‰'],
            '005490.KS': ['POSCO', 'í¬ìŠ¤ì½”', 'ì² ê°•', 'ìŠ¤í‹¸'],
            '051910.KS': ['LGí™”í•™', 'LG Chem', 'ë°°í„°ë¦¬', 'ì „ê¸°ì°¨ë°°í„°ë¦¬'],
            '207940.KS': ['ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'Samsung Biologics', 'ë°”ì´ì˜¤'],
            '005380.KS': ['í˜„ëŒ€ì°¨', 'Hyundai Motor', 'ìë™ì°¨', 'ì „ê¸°ì°¨'],
            '006400.KS': ['ì‚¼ì„±SDI', 'Samsung SDI', 'ë°°í„°ë¦¬', 'ESS']
        }

    def setup_reddit(self):
        """Reddit API ì„¤ì •"""
        try:
            # Reddit ë¬´ë£Œ API ì‚¬ìš© (read-only)
            self.reddit = praw.Reddit(
                client_id=os.getenv('REDDIT_CLIENT_ID', 'temp_client_id'),
                client_secret=os.getenv('REDDIT_CLIENT_SECRET', 'temp_secret'),
                user_agent='StockNewsCollector/1.0',
                check_for_async=False
            )
        except Exception as e:
            logging.warning(f"Reddit API setup failed: {e}")
            self.reddit = None

    def get_newsapi_articles(self, query, language='en', page_size=20):
        """NewsAPI.orgì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        if not self.newsapi_key:
            return []

        try:
            url = 'https://newsapi.org/v2/everything'
            params = {
                'q': query,
                'language': language,
                'sortBy': 'publishedAt',
                'pageSize': page_size,
                'from': (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),
                'apiKey': self.newsapi_key
            }

            response = requests.get(url, params=params, timeout=10)

            if response.status_code == 200:
                data = response.json()
                return data.get('articles', [])
            else:
                logging.error(f"NewsAPI error: {response.status_code}")
                return []

        except Exception as e:
            logging.error(f"Error fetching NewsAPI articles: {e}")
            return []

    def get_yahoo_finance_news(self, symbol):
        """Yahoo Financeì—ì„œ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
        try:
            # í•œêµ­ ì£¼ì‹ì€ ì‹¬ë³¼ ë³€í™˜
            if symbol.endswith('.KS') or symbol.endswith('.KQ'):
                search_symbol = symbol.replace('.KS', '.KS').replace('.KQ', '.KQ')
            else:
                search_symbol = symbol

            url = f"https://finance.yahoo.com/quote/{search_symbol}/news"

            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code != 200:
                return []

            soup = BeautifulSoup(response.content, 'html.parser')
            articles = []

            # Yahoo Finance ë‰´ìŠ¤ í•­ëª© ì°¾ê¸°
            news_items = soup.find_all('div', {'class': re.compile(r'.*stream-item.*|.*news-item.*')})

            for item in news_items[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    title_elem = item.find('h3') or item.find('a')
                    if not title_elem:
                        continue

                    title = title_elem.get_text(strip=True)

                    # ë§í¬ ì°¾ê¸°
                    link_elem = item.find('a', href=True)
                    link = ''
                    if link_elem:
                        link = link_elem['href']
                        if link.startswith('/'):
                            link = urljoin('https://finance.yahoo.com', link)

                    # ì‹œê°„ ì •ë³´
                    time_elem = item.find('div', {'class': re.compile(r'.*time.*|.*date.*')})
                    published_time = datetime.now().isoformat()

                    if time_elem:
                        time_text = time_elem.get_text(strip=True)
                        # ê°„ë‹¨í•œ ì‹œê°„ íŒŒì‹± (Yahooì˜ ìƒëŒ€ì‹œê°„)
                        if 'hour' in time_text or 'minute' in time_text:
                            published_time = (datetime.now() - timedelta(hours=1)).isoformat()

                    articles.append({
                        'title': title,
                        'description': title,  # YahooëŠ” ì œëª©ë§Œ ì‚¬ìš©
                        'url': link,
                        'publishedAt': published_time,
                        'source': {'name': 'Yahoo Finance'},
                        'content': ''
                    })

                except Exception as e:
                    continue

            return articles

        except Exception as e:
            logging.error(f"Error scraping Yahoo Finance news for {symbol}: {e}")
            return []

    def get_korean_rss_news(self):
        """í•œêµ­ RSS ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []

        for source_name, rss_url in self.korean_rss_feeds.items():
            try:
                # RSSê°€ ì•„ë‹Œ HTML í˜ì´ì§€ì¸ ê²½ìš° ìŠ¤í¬ë˜í•‘
                if 'naver.com' in rss_url:
                    articles = self.scrape_naver_finance(rss_url)
                else:
                    feed = feedparser.parse(rss_url)
                    articles = []

                    for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
                        try:
                            published_time = datetime.now().isoformat()

                            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                                published_time = datetime(*entry.published_parsed[:6]).isoformat()

                            articles.append({
                                'title': entry.get('title', ''),
                                'description': entry.get('summary', ''),
                                'url': entry.get('link', ''),
                                'publishedAt': published_time,
                                'source': {'name': source_name},
                                'content': entry.get('description', '')
                            })

                        except Exception as e:
                            continue

                all_articles.extend(articles)

            except Exception as e:
                logging.error(f"Error fetching RSS from {source_name}: {e}")
                continue

        return all_articles

    def scrape_naver_finance(self, url):
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)

            if response.status_code != 200:
                return []

            soup = BeautifulSoup(response.content, 'html.parser')
            articles = []

            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í•­ëª© ì°¾ê¸°
            news_items = soup.find_all('tr') + soup.find_all('dl', {'class': 'newsList'})

            for item in news_items[:10]:
                try:
                    # ì œëª© ì°¾ê¸°
                    title_elem = item.find('a', {'class': re.compile(r'.*title.*|.*tit.*')})
                    if not title_elem:
                        title_elem = item.find('a')

                    if not title_elem:
                        continue

                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')

                    if link and link.startswith('/'):
                        link = urljoin('https://finance.naver.com', link)

                    # ì‹œê°„ ì •ë³´
                    time_elem = item.find('span', {'class': re.compile(r'.*date.*|.*time.*')})
                    published_time = datetime.now().isoformat()

                    articles.append({
                        'title': title,
                        'description': title,
                        'url': link,
                        'publishedAt': published_time,
                        'source': {'name': 'Naver Finance'},
                        'content': ''
                    })

                except Exception as e:
                    continue

            return articles

        except Exception as e:
            logging.error(f"Error scraping Naver Finance: {e}")
            return []

    def get_reddit_posts(self, symbol, subreddit_list=['stocks', 'investing', 'SecurityAnalysis']):
        """Redditì—ì„œ ì£¼ì‹ ê´€ë ¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        if not self.reddit:
            return []

        articles = []

        try:
            for subreddit_name in subreddit_list:
                try:
                    subreddit = self.reddit.subreddit(subreddit_name)

                    # ì‹¬ë³¼ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
                    keywords = self.symbol_keywords.get(symbol, [symbol])

                    for keyword in keywords[:2]:  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œ
                        try:
                            for post in subreddit.search(keyword, time_filter='day', limit=5):
                                articles.append({
                                    'title': post.title,
                                    'description': post.selftext[:200] if post.selftext else post.title,
                                    'url': f"https://reddit.com{post.permalink}",
                                    'publishedAt': datetime.fromtimestamp(post.created_utc).isoformat(),
                                    'source': {'name': f'Reddit r/{subreddit_name}'},
                                    'content': post.selftext,
                                    'score': post.score,
                                    'num_comments': post.num_comments
                                })
                        except Exception as e:
                            continue

                except Exception as e:
                    logging.error(f"Error accessing subreddit {subreddit_name}: {e}")
                    continue

        except Exception as e:
            logging.error(f"Error fetching Reddit posts: {e}")

        return articles

class MCPNewsAnalyzer:
    """MCP news_analyzer ì—°ë™"""

    def __init__(self):
        self.mcp_path = Path('mcp/tools/news_analyzer/runner.py')

    def analyze_sentiment(self, title, content="", source=""):
        """MCP news_analyzerë¡œ ê°ì„± ë¶„ì„"""
        try:
            # MCPê°€ ì—†ëŠ” ê²½ìš° ë”ë¯¸ ë¶„ì„ê¸° ì‚¬ìš©
            if not self.mcp_path.exists():
                return self.dummy_sentiment_analysis(title, content)

            # MCP í˜¸ì¶œ
            cmd = [
                'python', str(self.mcp_path),
                '--text', f"{title} {content}",
                '--source', source
            ]

            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                try:
                    analysis = json.loads(result.stdout)
                    return {
                        'sentiment_score': analysis.get('sentiment_score', 0),
                        'sentiment_label': analysis.get('sentiment_label', 'neutral'),
                        'confidence': analysis.get('confidence', 0.5),
                        'keywords': analysis.get('keywords', [])
                    }
                except json.JSONDecodeError:
                    pass

            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë¶„ì„ê¸° ì‚¬ìš©
            return self.dummy_sentiment_analysis(title, content)

        except Exception as e:
            logging.error(f"Error analyzing sentiment: {e}")
            return self.dummy_sentiment_analysis(title, content)

    def dummy_sentiment_analysis(self, title, content=""):
        """ë”ë¯¸ ê°ì„± ë¶„ì„ê¸° (MCP ì—†ì„ ë•Œ ì‚¬ìš©)"""
        text = f"{title} {content}".lower()

        # ê¸ì • í‚¤ì›Œë“œ
        positive_words = [
            'buy', 'bullish', 'growth', 'profit', 'gain', 'rise', 'increase',
            'strong', 'beat', 'exceed', 'outperform', 'positive', 'good',
            'ìƒìŠ¹', 'í˜¸ì¬', 'ê¸‰ë“±', 'ì„±ì¥', 'ìˆ˜ìµ', 'ê¸ì •', 'ë§¤ìˆ˜'
        ]

        # ë¶€ì • í‚¤ì›Œë“œ
        negative_words = [
            'sell', 'bearish', 'loss', 'fall', 'decrease', 'decline', 'drop',
            'weak', 'miss', 'underperform', 'negative', 'bad', 'crash',
            'í•˜ë½', 'ì•…ì¬', 'ê¸‰ë½', 'ì†ì‹¤', 'ë¶€ì •', 'ë§¤ë„'
        ]

        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)

        if positive_count > negative_count:
            sentiment_score = min(0.8, 0.5 + (positive_count - negative_count) * 0.1)
            sentiment_label = 'positive'
        elif negative_count > positive_count:
            sentiment_score = max(0.2, 0.5 - (negative_count - positive_count) * 0.1)
            sentiment_label = 'negative'
        else:
            sentiment_score = 0.5
            sentiment_label = 'neutral'

        return {
            'sentiment_score': sentiment_score,
            'sentiment_label': sentiment_label,
            'confidence': 0.6,
            'keywords': []
        }

class NewsSentimentCollector:
    """ë‰´ìŠ¤ ê°ì„± ìˆ˜ì§‘ê¸° ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.news_manager = NewsSourceManager()
        self.sentiment_analyzer = MCPNewsAnalyzer()

        self.setup_directories()
        self.setup_logging()
        self.setup_database()

        self.running = False

        # ìˆ˜ì§‘ ëŒ€ìƒ ì‹¬ë³¼ (watchlistì—ì„œ ë¡œë“œ)
        self.symbols = self.load_watchlist()

    def setup_directories(self):
        """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
        today = datetime.now().strftime('%Y-%m-%d')
        self.news_dir = Path(f'data/news/{today}')
        self.news_dir.mkdir(parents=True, exist_ok=True)

    def setup_logging(self):
        """ë¡œê¹… ì„¤ì •"""
        today = datetime.now().strftime('%Y-%m-%d')
        log_file = f'logs/news_collector_{today}.log'

        Path('logs').mkdir(exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )

        self.logger = logging.getLogger(__name__)

    def setup_database(self):
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
        self.db_path = "news.db"
        conn = sqlite3.connect(self.db_path)

        conn.execute('''
            CREATE TABLE IF NOT EXISTS news_articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                hash TEXT UNIQUE,
                symbol TEXT,
                title TEXT,
                description TEXT,
                url TEXT,
                source TEXT,
                published_at DATETIME,
                collected_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                sentiment_score REAL,
                sentiment_label TEXT,
                confidence REAL,
                keywords TEXT
            )
        ''')

        conn.execute('''
            CREATE TABLE IF NOT EXISTS symbol_sentiment (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                symbol TEXT,
                date DATE,
                avg_sentiment REAL,
                article_count INTEGER,
                positive_count INTEGER,
                negative_count INTEGER,
                neutral_count INTEGER,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        conn.commit()
        conn.close()

    def load_watchlist(self):
        """ê°ì‹œ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ ë¡œë“œ"""
        try:
            if Path('watchlist.txt').exists():
                with open('watchlist.txt', 'r') as f:
                    symbols = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                return symbols
            else:
                # ê¸°ë³¸ ì‹¬ë³¼
                return ['AAPL', 'MSFT', 'GOOGL', 'TSLA', '005930.KS', '000660.KS']

        except Exception as e:
            self.logger.error(f"Error loading watchlist: {e}")
            return ['AAPL', 'MSFT', 'GOOGL', 'TSLA']

    def calculate_content_hash(self, article):
        """ê¸°ì‚¬ ë‚´ìš© í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ì œê±°ìš©)"""
        content = f"{article.get('title', '')}{article.get('url', '')}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()

    def is_relevant_to_symbol(self, article, symbol):
        """ê¸°ì‚¬ê°€ íŠ¹ì • ì‹¬ë³¼ê³¼ ê´€ë ¨ìˆëŠ”ì§€ í™•ì¸"""
        text = f"{article.get('title', '')} {article.get('description', '')}".lower()

        # ì‹¬ë³¼ í‚¤ì›Œë“œ í™•ì¸
        keywords = self.news_manager.symbol_keywords.get(symbol, [symbol.replace('.KS', '').replace('.KQ', '')])

        for keyword in keywords:
            if keyword.lower() in text:
                return True

        return False

    def collect_news_for_symbol(self, symbol):
        """íŠ¹ì • ì‹¬ë³¼ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_articles = []

        try:
            # 1. NewsAPIì—ì„œ ìˆ˜ì§‘
            if self.news_manager.newsapi_key:
                keywords = self.news_manager.symbol_keywords.get(symbol, [symbol])
                for keyword in keywords[:2]:  # ìµœëŒ€ 2ê°œ í‚¤ì›Œë“œ
                    articles = self.news_manager.get_newsapi_articles(keyword)
                    all_articles.extend(articles)

            # 2. Yahoo Financeì—ì„œ ìˆ˜ì§‘
            yahoo_articles = self.news_manager.get_yahoo_finance_news(symbol)
            all_articles.extend(yahoo_articles)

            # 3. Redditì—ì„œ ìˆ˜ì§‘
            reddit_articles = self.news_manager.get_reddit_posts(symbol)
            all_articles.extend(reddit_articles)

            # 4. í•œêµ­ ì£¼ì‹ì¸ ê²½ìš° í•œêµ­ ë‰´ìŠ¤ë„ ìˆ˜ì§‘
            if symbol.endswith('.KS') or symbol.endswith('.KQ'):
                korean_articles = self.news_manager.get_korean_rss_news()
                # ê´€ë ¨ì„± í•„í„°ë§
                relevant_articles = [
                    article for article in korean_articles
                    if self.is_relevant_to_symbol(article, symbol)
                ]
                all_articles.extend(relevant_articles)

            # ê´€ë ¨ì„± ì¬í™•ì¸ ë° í•„í„°ë§
            relevant_articles = []
            for article in all_articles:
                if self.is_relevant_to_symbol(article, symbol):
                    relevant_articles.append(article)

            self.logger.info(f"Collected {len(relevant_articles)} relevant articles for {symbol}")
            return relevant_articles

        except Exception as e:
            self.logger.error(f"Error collecting news for {symbol}: {e}")
            return []

    def save_article_to_db(self, symbol, article, sentiment_data):
        """ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            conn = sqlite3.connect(self.db_path)

            article_hash = self.calculate_content_hash(article)

            # ì¤‘ë³µ í™•ì¸
            cursor = conn.execute('SELECT id FROM news_articles WHERE hash = ?', (article_hash,))
            if cursor.fetchone():
                conn.close()
                return False  # ì´ë¯¸ ì¡´ì¬

            # ê¸°ì‚¬ ì €ì¥
            conn.execute('''
                INSERT INTO news_articles
                (hash, symbol, title, description, url, source, published_at,
                 sentiment_score, sentiment_label, confidence, keywords)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article_hash,
                symbol,
                article.get('title', ''),
                article.get('description', ''),
                article.get('url', ''),
                article.get('source', {}).get('name', ''),
                article.get('publishedAt', datetime.now().isoformat()),
                sentiment_data['sentiment_score'],
                sentiment_data['sentiment_label'],
                sentiment_data['confidence'],
                json.dumps(sentiment_data['keywords'])
            ))

            conn.commit()
            conn.close()
            return True

        except Exception as e:
            self.logger.error(f"Error saving article to DB: {e}")
            return False

    def save_article_to_json(self, symbol, articles_with_sentiment):
        """ê¸°ì‚¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            safe_symbol = symbol.replace('.', '_')
            json_file = self.news_dir / f'{safe_symbol}_news.json'

            # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
            existing_data = []
            if json_file.exists():
                try:
                    with open(json_file, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except:
                    existing_data = []

            # ìƒˆ ë°ì´í„° ì¶”ê°€
            existing_data.extend(articles_with_sentiment)

            # ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ì¤€)
            seen_hashes = set()
            unique_data = []

            for article in existing_data:
                article_hash = self.calculate_content_hash(article)
                if article_hash not in seen_hashes:
                    seen_hashes.add(article_hash)
                    unique_data.append(article)

            # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
            unique_data = unique_data[-100:]

            # íŒŒì¼ ì €ì¥
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(unique_data, f, indent=2, ensure_ascii=False)

        except Exception as e:
            self.logger.error(f"Error saving articles to JSON: {e}")

    def update_symbol_sentiment_summary(self, symbol):
        """ì‹¬ë³¼ë³„ ê°ì„± ìš”ì•½ ì—…ë°ì´íŠ¸"""
        try:
            conn = sqlite3.connect(self.db_path)
            today = datetime.now().date()

            # ì˜¤ëŠ˜ì˜ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ ê°ì„± í†µê³„
            cursor = conn.execute('''
                SELECT
                    AVG(sentiment_score) as avg_sentiment,
                    COUNT(*) as total_count,
                    SUM(CASE WHEN sentiment_label = 'positive' THEN 1 ELSE 0 END) as positive_count,
                    SUM(CASE WHEN sentiment_label = 'negative' THEN 1 ELSE 0 END) as negative_count,
                    SUM(CASE WHEN sentiment_label = 'neutral' THEN 1 ELSE 0 END) as neutral_count
                FROM news_articles
                WHERE symbol = ? AND DATE(published_at) = ?
            ''', (symbol, today))

            result = cursor.fetchone()

            if result and result[1] > 0:  # ê¸°ì‚¬ê°€ ìˆëŠ” ê²½ìš°
                avg_sentiment, total_count, positive_count, negative_count, neutral_count = result

                # ê¸°ì¡´ ë ˆì½”ë“œ ì‚­ì œ í›„ ìƒˆë¡œ ì‚½ì…
                conn.execute('DELETE FROM symbol_sentiment WHERE symbol = ? AND date = ?', (symbol, today))

                conn.execute('''
                    INSERT INTO symbol_sentiment
                    (symbol, date, avg_sentiment, article_count, positive_count, negative_count, neutral_count)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (symbol, today, avg_sentiment, total_count, positive_count, negative_count, neutral_count))

                conn.commit()

            conn.close()

        except Exception as e:
            self.logger.error(f"Error updating sentiment summary for {symbol}: {e}")

    def get_symbol_sentiment_score(self, symbol):
        """ì‹¬ë³¼ì˜ ë‰´ìŠ¤ ê°ì„± ì ìˆ˜ ì¡°íšŒ (0-100)"""
        try:
            conn = sqlite3.connect(self.db_path)
            today = datetime.now().date()

            cursor = conn.execute('''
                SELECT avg_sentiment, article_count, positive_count, negative_count
                FROM symbol_sentiment
                WHERE symbol = ? AND date = ?
            ''', (symbol, today))

            result = cursor.fetchone()
            conn.close()

            if result:
                avg_sentiment, article_count, positive_count, negative_count = result

                # ê°ì„± ì ìˆ˜ë¥¼ 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
                # 0.5 (ì¤‘ë¦½) = 50ì , 1.0 (ë§¤ìš° ê¸ì •) = 100ì , 0.0 (ë§¤ìš° ë¶€ì •) = 0ì 
                base_score = avg_sentiment * 100

                # ê¸°ì‚¬ ìˆ˜ì— ë”°ë¥¸ ì‹ ë¢°ë„ ì¡°ì •
                confidence_multiplier = min(1.0, article_count / 10)  # 10ê°œ ì´ìƒì´ë©´ 100% ì‹ ë¢°

                # ê¸ì •/ë¶€ì • ë¹„ìœ¨ ê³ ë ¤
                if article_count > 0:
                    sentiment_ratio = (positive_count - negative_count) / article_count
                    ratio_bonus = sentiment_ratio * 10  # ìµœëŒ€ Â±10ì 

                    final_score = base_score + ratio_bonus
                    final_score = max(0, min(100, final_score))
                    final_score *= confidence_multiplier

                    return {
                        'score': final_score,
                        'article_count': article_count,
                        'avg_sentiment': avg_sentiment,
                        'confidence': confidence_multiplier
                    }

            return {'score': 50, 'article_count': 0, 'avg_sentiment': 0.5, 'confidence': 0}

        except Exception as e:
            self.logger.error(f"Error getting sentiment score for {symbol}: {e}")
            return {'score': 50, 'article_count': 0, 'avg_sentiment': 0.5, 'confidence': 0}

    def clean_old_news(self):
        """24ì‹œê°„ ì´ìƒ ëœ ë‰´ìŠ¤ ì •ë¦¬"""
        try:
            conn = sqlite3.connect(self.db_path)

            # 24ì‹œê°„ ì´ìƒ ëœ ê¸°ì‚¬ ì‚­ì œ
            cutoff_time = datetime.now() - timedelta(hours=24)

            cursor = conn.execute('DELETE FROM news_articles WHERE published_at < ?', (cutoff_time.isoformat(),))
            deleted_count = cursor.rowcount

            # ì˜¤ë˜ëœ ê°ì„± ìš”ì•½ ì‚­ì œ (7ì¼ ì´ìƒ)
            cutoff_date = (datetime.now() - timedelta(days=7)).date()
            conn.execute('DELETE FROM symbol_sentiment WHERE date < ?', (cutoff_date,))

            conn.commit()
            conn.close()

            if deleted_count > 0:
                self.logger.info(f"Cleaned {deleted_count} old news articles")

        except Exception as e:
            self.logger.error(f"Error cleaning old news: {e}")

    def run_collection_cycle(self):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‚¬ì´í´ ì‹¤í–‰"""
        try:
            self.logger.info("Starting news collection cycle...")

            for symbol in self.symbols:
                try:
                    self.logger.info(f"Collecting news for {symbol}...")

                    # ë‰´ìŠ¤ ìˆ˜ì§‘
                    articles = self.collect_news_for_symbol(symbol)

                    if not articles:
                        continue

                    # ê°ì„± ë¶„ì„ ë° ì €ì¥
                    articles_with_sentiment = []
                    new_articles_count = 0

                    for article in articles:
                        try:
                            # ê°ì„± ë¶„ì„
                            sentiment_data = self.sentiment_analyzer.analyze_sentiment(
                                article.get('title', ''),
                                article.get('description', ''),
                                article.get('source', {}).get('name', '')
                            )

                            # ê¸°ì‚¬ì— ê°ì„± ë°ì´í„° ì¶”ê°€
                            article_with_sentiment = article.copy()
                            article_with_sentiment.update(sentiment_data)
                            articles_with_sentiment.append(article_with_sentiment)

                            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                            if self.save_article_to_db(symbol, article, sentiment_data):
                                new_articles_count += 1

                            # API ì œí•œ ë°©ì§€
                            time.sleep(0.1)

                        except Exception as e:
                            self.logger.error(f"Error processing article: {e}")
                            continue

                    # JSON íŒŒì¼ ì €ì¥
                    if articles_with_sentiment:
                        self.save_article_to_json(symbol, articles_with_sentiment)

                    # ê°ì„± ìš”ì•½ ì—…ë°ì´íŠ¸
                    self.update_symbol_sentiment_summary(symbol)

                    self.logger.info(f"Processed {len(articles)} articles for {symbol}, {new_articles_count} new")

                except Exception as e:
                    self.logger.error(f"Error collecting news for {symbol}: {e}")
                    continue

            # ì˜¤ë˜ëœ ë‰´ìŠ¤ ì •ë¦¬
            self.clean_old_news()

            self.logger.info("News collection cycle completed")

        except Exception as e:
            self.logger.error(f"Error in news collection cycle: {e}")

    def display_sentiment_summary(self):
        """ê°ì„± ìš”ì•½ ì¶œë ¥"""
        try:
            print("\n" + "=" * 60)
            print("ğŸ“° ë‰´ìŠ¤ ê°ì„± ë¶„ì„ í˜„í™©")
            print("=" * 60)

            for symbol in self.symbols:
                sentiment_data = self.get_symbol_sentiment_score(symbol)

                score = sentiment_data['score']
                article_count = sentiment_data['article_count']

                if article_count > 0:
                    if score >= 70:
                        trend = "ğŸŸ¢ ê¸ì •ì "
                    elif score <= 30:
                        trend = "ğŸ”´ ë¶€ì •ì "
                    else:
                        trend = "ğŸŸ¡ ì¤‘ë¦½ì "

                    print(f"{symbol}: {score:.1f}ì  {trend} ({article_count}ê°œ ê¸°ì‚¬)")
                else:
                    print(f"{symbol}: ë‰´ìŠ¤ ì—†ìŒ")

            print("=" * 60)

        except Exception as e:
            self.logger.error(f"Error displaying sentiment summary: {e}")

    def start_collection(self, interval_minutes=5):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘"""
        self.running = True

        self.logger.info("ğŸš€ Starting news sentiment collector...")
        self.logger.info(f"ğŸ“Š Monitoring {len(self.symbols)} symbols")
        self.logger.info(f"â° Collection interval: {interval_minutes} minutes")

        try:
            while self.running:
                start_time = time.time()

                # ìˆ˜ì§‘ ì‚¬ì´í´ ì‹¤í–‰
                self.run_collection_cycle()

                # ê°ì„± ìš”ì•½ ì¶œë ¥
                self.display_sentiment_summary()

                # ì¸í„°ë²Œ ìœ ì§€
                elapsed = time.time() - start_time
                sleep_time = max(0, interval_minutes * 60 - elapsed)

                if sleep_time > 0:
                    self.logger.info(f"Sleeping for {sleep_time/60:.1f} minutes...")
                    time.sleep(sleep_time)

        except KeyboardInterrupt:
            self.logger.info("Collection stopped by user")
        except Exception as e:
            self.logger.error(f"Fatal error in collection loop: {e}")
        finally:
            self.running = False

    def stop_collection(self):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ì§€"""
        self.running = False
        self.logger.info("Stopping news collection...")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“° ë‰´ìŠ¤ ê°ì„± ìˆ˜ì§‘ê¸°")
    print("=" * 50)
    print("ì§€ì› ì†ŒìŠ¤:")
    print("- NewsAPI.org (API í‚¤ í•„ìš”)")
    print("- Yahoo Finance")
    print("- Reddit")
    print("- í•œêµ­ ë‰´ìŠ¤ RSS")
    print("=" * 50)

    # API í‚¤ í™•ì¸
    newsapi_key = os.getenv('NEWSAPI_KEY')
    if newsapi_key:
        print(f"âœ… NewsAPI í‚¤ ì„¤ì •ë¨")
    else:
        print("âš ï¸ NewsAPI í‚¤ ì—†ìŒ (NEWSAPI_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì •)")

    try:
        collector = NewsSentimentCollector()
        collector.start_collection(interval_minutes=5)

    except Exception as e:
        print(f"âŒ Error starting collector: {e}")

if __name__ == "__main__":
    main()